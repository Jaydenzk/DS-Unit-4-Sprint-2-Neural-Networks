{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_433_Keras_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaydenzk/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pBQsZEJmubLs"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "# Neural Network Framework (Keras)\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignmnet 3*\n",
        "\n",
        "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
        "\n",
        "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
        "- Normalize the data (all features should have roughly the same scale)\n",
        "- Import the type of model and layers that you will need from Keras.\n",
        "- Instantiate a model object and use `model.add()` to add layers to your model\n",
        "- Since this is a regression model you will have a single output node in the final layer.\n",
        "- Use activation functions that are appropriate for this task\n",
        "- Compile your model\n",
        "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
        "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
        "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8NLTAR87uYJ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "938ffbe9-3196-43cd-f574-75b0c7f3b33c"
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "# Load Boston housing dataset\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "from tensorflow.keras.utils import normalize\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "(404, 13) (404,) (102, 13) (102,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u3ijMRVVTbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalzie the data\n",
        "x_train = normalize(x_train, axis=1)\n",
        "x_test = normalize(x_test, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20fikcvqVWYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "7be72acd-835d-4f38-d543-3d046e310f10"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "np.random.seed(812)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Dense(16, input_dim=13, activation='relu'))\n",
        "\n",
        "# Hidden Layer\n",
        "model.add(Dense(16, activation='sigmoid'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(16, activation='linear'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 16)                224       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 1,057\n",
            "Trainable params: 1,057\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLMb-falVWW2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3eaa3d43-d84f-4ee7-b8d3-5b824f85df16"
      },
      "source": [
        "# Hyper parameter\n",
        "batch_size = 32\n",
        "epochs = 1000\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=.1, verbose=0)\n",
        "# validation_split - reserve some data from dataset\n",
        "# verbose shows the print statements\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(f'{model.metrics_names[1]}: {scores[1]}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102/102 [==============================] - 0s 91us/sample - loss: 30.7300 - mean_squared_error: 30.7300\n",
            "mean_squared_error: 30.729955673217773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HrKd75gVWVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "4cf80c1c-6aec-4b90-b0f4-7033690d53cd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ((ax1,ax2)) = plt.subplots(1,2)\n",
        "ax1.plot(history.history['loss'], color = 'r')\n",
        "ax1.set_title(\"Loss\")\n",
        "ax2.plot(history.history['mean_squared_error'], color = 'g')\n",
        "ax2.set_title(\"MSE\");"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxGtiX9-VWTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c604586d-a99e-4213-e8f0-43cebfe9a16f"
      },
      "source": [
        "# Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Instantiate Model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Fit model\n",
        "lr_model.fit(x_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = lr_model.predict(x_test)\n",
        "\n",
        "# Accuracy\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Linear Regression MSE:\", mse)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression MSE: 18.165510493496484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SfcFnOONyuNm"
      },
      "source": [
        "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
        "\n",
        "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
        "- Make sure to one-hot encode your category labels\n",
        "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
        "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szi6-IpuzaH1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad01e50e-1860-40fb-f16e-7c393000b4a9"
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "# Load bonston housing dataset\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import normalize\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqVHXGWWVsWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Parameters\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Lwpf5pVu5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape the data (28*28 = 784)\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "# X Variable Types\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "# one-hot encode category labels for softmax\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arb7nrnXVu3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "5b0d8f7c-818c-4457-f86c-536d13c5a99f"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "np.random.seed(812)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Dense(16, input_dim=784, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# Hidden Layer\n",
        "model.add(Dense(16, activation='sigmoid'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "\n",
        "# Compile\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 16)                12560     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 13,274\n",
            "Trainable params: 13,274\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt-FEoXDVu1s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a9a0671-e13a-497a-fd8f-0afc7e8c1ce4"
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=.1, verbose=1)\n",
        "# validation_split - reserve some data from dataset\n",
        "# verbose shows the print statements\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(f'{model.metrics_names[1]}: {scores[1]*100}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/100\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 1.2937 - acc: 0.4442 - val_loss: 0.9970 - val_acc: 0.5760\n",
            "Epoch 2/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.9357 - acc: 0.6129 - val_loss: 0.8565 - val_acc: 0.6613\n",
            "Epoch 3/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.8661 - acc: 0.6530 - val_loss: 0.8606 - val_acc: 0.6700\n",
            "Epoch 4/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.7938 - acc: 0.6978 - val_loss: 0.7635 - val_acc: 0.7190\n",
            "Epoch 5/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.7630 - acc: 0.7065 - val_loss: 0.7082 - val_acc: 0.7332\n",
            "Epoch 6/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.7653 - acc: 0.7117 - val_loss: 0.7212 - val_acc: 0.7427\n",
            "Epoch 7/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.7424 - acc: 0.7139 - val_loss: 0.7311 - val_acc: 0.7202\n",
            "Epoch 8/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.7157 - acc: 0.7244 - val_loss: 0.7279 - val_acc: 0.7087\n",
            "Epoch 9/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.7068 - acc: 0.7386 - val_loss: 0.6724 - val_acc: 0.7625\n",
            "Epoch 10/100\n",
            "54000/54000 [==============================] - 3s 64us/sample - loss: 0.6897 - acc: 0.7389 - val_loss: 0.6570 - val_acc: 0.7667\n",
            "Epoch 11/100\n",
            "54000/54000 [==============================] - 3s 64us/sample - loss: 0.6748 - acc: 0.7475 - val_loss: 0.6678 - val_acc: 0.7453\n",
            "Epoch 12/100\n",
            "54000/54000 [==============================] - 4s 68us/sample - loss: 0.6738 - acc: 0.7429 - val_loss: 0.6601 - val_acc: 0.7563\n",
            "Epoch 13/100\n",
            "54000/54000 [==============================] - 3s 64us/sample - loss: 0.6795 - acc: 0.7461 - val_loss: 0.7166 - val_acc: 0.7075\n",
            "Epoch 14/100\n",
            "54000/54000 [==============================] - 4s 65us/sample - loss: 0.6849 - acc: 0.7419 - val_loss: 0.6565 - val_acc: 0.7493\n",
            "Epoch 15/100\n",
            "54000/54000 [==============================] - 3s 62us/sample - loss: 0.6698 - acc: 0.7471 - val_loss: 0.6513 - val_acc: 0.7555\n",
            "Epoch 16/100\n",
            "54000/54000 [==============================] - 3s 61us/sample - loss: 0.6443 - acc: 0.7570 - val_loss: 0.6403 - val_acc: 0.7683\n",
            "Epoch 17/100\n",
            "54000/54000 [==============================] - 3s 61us/sample - loss: 0.6510 - acc: 0.7512 - val_loss: 0.6486 - val_acc: 0.7620\n",
            "Epoch 18/100\n",
            "54000/54000 [==============================] - 3s 64us/sample - loss: 0.6611 - acc: 0.7509 - val_loss: 0.6282 - val_acc: 0.7728\n",
            "Epoch 19/100\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.6340 - acc: 0.7630 - val_loss: 0.6029 - val_acc: 0.7703\n",
            "Epoch 20/100\n",
            "54000/54000 [==============================] - 3s 57us/sample - loss: 0.6129 - acc: 0.7701 - val_loss: 0.6143 - val_acc: 0.7685\n",
            "Epoch 21/100\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.6614 - acc: 0.7479 - val_loss: 0.7031 - val_acc: 0.7512\n",
            "Epoch 22/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6739 - acc: 0.7517 - val_loss: 0.6511 - val_acc: 0.7588\n",
            "Epoch 23/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6302 - acc: 0.7667 - val_loss: 0.6776 - val_acc: 0.7565\n",
            "Epoch 24/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6735 - acc: 0.7488 - val_loss: 0.7077 - val_acc: 0.7177\n",
            "Epoch 25/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6746 - acc: 0.7509 - val_loss: 0.6553 - val_acc: 0.7597\n",
            "Epoch 26/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6451 - acc: 0.7551 - val_loss: 0.6186 - val_acc: 0.7535\n",
            "Epoch 27/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6254 - acc: 0.7594 - val_loss: 0.6712 - val_acc: 0.7427\n",
            "Epoch 28/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6132 - acc: 0.7699 - val_loss: 0.6065 - val_acc: 0.7742\n",
            "Epoch 29/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6026 - acc: 0.7707 - val_loss: 0.6129 - val_acc: 0.7810\n",
            "Epoch 30/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6194 - acc: 0.7673 - val_loss: 0.6617 - val_acc: 0.7657\n",
            "Epoch 31/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6370 - acc: 0.7602 - val_loss: 0.6696 - val_acc: 0.7623\n",
            "Epoch 32/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6537 - acc: 0.7508 - val_loss: 0.6020 - val_acc: 0.7762\n",
            "Epoch 33/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5875 - acc: 0.7767 - val_loss: 0.5900 - val_acc: 0.7850\n",
            "Epoch 34/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5887 - acc: 0.7834 - val_loss: 0.5874 - val_acc: 0.7720\n",
            "Epoch 35/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5863 - acc: 0.7750 - val_loss: 0.5805 - val_acc: 0.7777\n",
            "Epoch 36/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5950 - acc: 0.7757 - val_loss: 0.6006 - val_acc: 0.7795\n",
            "Epoch 37/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5818 - acc: 0.7817 - val_loss: 0.5675 - val_acc: 0.8020\n",
            "Epoch 38/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6217 - acc: 0.7733 - val_loss: 0.5923 - val_acc: 0.7895\n",
            "Epoch 39/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5919 - acc: 0.7848 - val_loss: 0.6121 - val_acc: 0.7790\n",
            "Epoch 40/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6205 - acc: 0.7730 - val_loss: 0.6217 - val_acc: 0.7780\n",
            "Epoch 41/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5942 - acc: 0.7814 - val_loss: 0.5881 - val_acc: 0.7882\n",
            "Epoch 42/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5982 - acc: 0.7771 - val_loss: 0.5847 - val_acc: 0.7907\n",
            "Epoch 43/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5716 - acc: 0.7916 - val_loss: 0.5579 - val_acc: 0.7900\n",
            "Epoch 44/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5796 - acc: 0.7832 - val_loss: 0.5784 - val_acc: 0.7790\n",
            "Epoch 45/100\n",
            "54000/54000 [==============================] - 3s 52us/sample - loss: 0.6236 - acc: 0.7676 - val_loss: 0.7231 - val_acc: 0.7320\n",
            "Epoch 46/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6344 - acc: 0.7691 - val_loss: 0.6467 - val_acc: 0.7708\n",
            "Epoch 47/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6357 - acc: 0.7656 - val_loss: 0.6126 - val_acc: 0.7740\n",
            "Epoch 48/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6136 - acc: 0.7705 - val_loss: 0.6205 - val_acc: 0.7677\n",
            "Epoch 49/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6101 - acc: 0.7724 - val_loss: 0.5890 - val_acc: 0.7810\n",
            "Epoch 50/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5899 - acc: 0.7783 - val_loss: 0.6034 - val_acc: 0.7727\n",
            "Epoch 51/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6007 - acc: 0.7763 - val_loss: 0.6034 - val_acc: 0.7698\n",
            "Epoch 52/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5795 - acc: 0.7786 - val_loss: 0.6088 - val_acc: 0.7693\n",
            "Epoch 53/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5785 - acc: 0.7861 - val_loss: 0.5740 - val_acc: 0.7863\n",
            "Epoch 54/100\n",
            "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5766 - acc: 0.7931 - val_loss: 0.5880 - val_acc: 0.7933\n",
            "Epoch 55/100\n",
            "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5603 - acc: 0.8016 - val_loss: 0.5805 - val_acc: 0.7733\n",
            "Epoch 56/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5864 - acc: 0.7858 - val_loss: 0.6457 - val_acc: 0.7485\n",
            "Epoch 57/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.6273 - acc: 0.7761 - val_loss: 0.5796 - val_acc: 0.7882\n",
            "Epoch 58/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5633 - acc: 0.7934 - val_loss: 0.5690 - val_acc: 0.7907\n",
            "Epoch 59/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5869 - acc: 0.7844 - val_loss: 0.6139 - val_acc: 0.7680\n",
            "Epoch 60/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5766 - acc: 0.7889 - val_loss: 0.5938 - val_acc: 0.7832\n",
            "Epoch 61/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5976 - acc: 0.7859 - val_loss: 0.5736 - val_acc: 0.7943\n",
            "Epoch 62/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5887 - acc: 0.7831 - val_loss: 0.5854 - val_acc: 0.7795\n",
            "Epoch 63/100\n",
            "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5652 - acc: 0.7939 - val_loss: 0.5619 - val_acc: 0.7955\n",
            "Epoch 64/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5634 - acc: 0.7961 - val_loss: 0.6213 - val_acc: 0.7737\n",
            "Epoch 65/100\n",
            "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5731 - acc: 0.7908 - val_loss: 0.5803 - val_acc: 0.7810\n",
            "Epoch 66/100\n",
            "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5818 - acc: 0.7866 - val_loss: 0.5921 - val_acc: 0.7842\n",
            "Epoch 67/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5803 - acc: 0.7904 - val_loss: 0.6020 - val_acc: 0.7802\n",
            "Epoch 68/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5831 - acc: 0.7920 - val_loss: 0.6352 - val_acc: 0.7627\n",
            "Epoch 69/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6014 - acc: 0.7778 - val_loss: 0.5896 - val_acc: 0.7828\n",
            "Epoch 70/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6046 - acc: 0.7720 - val_loss: 0.5963 - val_acc: 0.7808\n",
            "Epoch 71/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5762 - acc: 0.7889 - val_loss: 0.5708 - val_acc: 0.8060\n",
            "Epoch 72/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5866 - acc: 0.7911 - val_loss: 0.6390 - val_acc: 0.7737\n",
            "Epoch 73/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6079 - acc: 0.7749 - val_loss: 0.6352 - val_acc: 0.7697\n",
            "Epoch 74/100\n",
            "54000/54000 [==============================] - 3s 57us/sample - loss: 0.5745 - acc: 0.7977 - val_loss: 0.5527 - val_acc: 0.8050\n",
            "Epoch 75/100\n",
            "54000/54000 [==============================] - 3s 52us/sample - loss: 0.5622 - acc: 0.8033 - val_loss: 0.5657 - val_acc: 0.8003\n",
            "Epoch 76/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.6016 - acc: 0.7814 - val_loss: 0.6923 - val_acc: 0.7067\n",
            "Epoch 77/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5864 - acc: 0.7889 - val_loss: 0.5796 - val_acc: 0.7865\n",
            "Epoch 78/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5900 - acc: 0.7910 - val_loss: 0.5998 - val_acc: 0.7898\n",
            "Epoch 79/100\n",
            "54000/54000 [==============================] - 3s 57us/sample - loss: 0.5720 - acc: 0.7929 - val_loss: 0.5796 - val_acc: 0.7870\n",
            "Epoch 80/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5797 - acc: 0.7869 - val_loss: 0.5876 - val_acc: 0.7848\n",
            "Epoch 81/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5696 - acc: 0.7938 - val_loss: 0.5737 - val_acc: 0.8010\n",
            "Epoch 82/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5597 - acc: 0.7998 - val_loss: 0.5733 - val_acc: 0.8042\n",
            "Epoch 83/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5671 - acc: 0.7951 - val_loss: 0.5723 - val_acc: 0.7967\n",
            "Epoch 84/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5852 - acc: 0.7961 - val_loss: 0.5743 - val_acc: 0.8048\n",
            "Epoch 85/100\n",
            "54000/54000 [==============================] - 3s 53us/sample - loss: 0.5843 - acc: 0.7941 - val_loss: 0.6075 - val_acc: 0.7875\n",
            "Epoch 86/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.6022 - acc: 0.7835 - val_loss: 0.6001 - val_acc: 0.7822\n",
            "Epoch 87/100\n",
            "54000/54000 [==============================] - 3s 57us/sample - loss: 0.5812 - acc: 0.7918 - val_loss: 0.5837 - val_acc: 0.8040\n",
            "Epoch 88/100\n",
            "54000/54000 [==============================] - 3s 58us/sample - loss: 0.5499 - acc: 0.8076 - val_loss: 0.5578 - val_acc: 0.7985\n",
            "Epoch 89/100\n",
            "54000/54000 [==============================] - 3s 58us/sample - loss: 0.5688 - acc: 0.7991 - val_loss: 0.5932 - val_acc: 0.7960\n",
            "Epoch 90/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5634 - acc: 0.8080 - val_loss: 0.5547 - val_acc: 0.8095\n",
            "Epoch 91/100\n",
            "54000/54000 [==============================] - 3s 55us/sample - loss: 0.5798 - acc: 0.8016 - val_loss: 0.6103 - val_acc: 0.7883\n",
            "Epoch 92/100\n",
            "54000/54000 [==============================] - 3s 57us/sample - loss: 0.5940 - acc: 0.7827 - val_loss: 0.5749 - val_acc: 0.7972\n",
            "Epoch 93/100\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.5725 - acc: 0.7921 - val_loss: 0.5678 - val_acc: 0.7912\n",
            "Epoch 94/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5585 - acc: 0.7977 - val_loss: 0.5737 - val_acc: 0.7980\n",
            "Epoch 95/100\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.5538 - acc: 0.8012 - val_loss: 0.5496 - val_acc: 0.8048\n",
            "Epoch 96/100\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.5500 - acc: 0.8053 - val_loss: 0.5562 - val_acc: 0.8078\n",
            "Epoch 97/100\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.5567 - acc: 0.8024 - val_loss: 0.5675 - val_acc: 0.8022\n",
            "Epoch 98/100\n",
            "54000/54000 [==============================] - 3s 56us/sample - loss: 0.5710 - acc: 0.7922 - val_loss: 0.6074 - val_acc: 0.7790\n",
            "Epoch 99/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5722 - acc: 0.8022 - val_loss: 0.5796 - val_acc: 0.8020\n",
            "Epoch 100/100\n",
            "54000/54000 [==============================] - 3s 54us/sample - loss: 0.5599 - acc: 0.8035 - val_loss: 0.5583 - val_acc: 0.8037\n",
            "10000/10000 [==============================] - 0s 27us/sample - loss: 0.5829 - acc: 0.7965\n",
            "acc: 79.6500027179718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zv_3xNMjzdLI"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
        "- Use Cross Validation techniques to get more consistent results with your model.\n",
        "- Use GridSearchCV to try different combinations of hyperparameters. \n",
        "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
      ]
    }
  ]
}